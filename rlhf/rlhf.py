"""
PART 5: Reinforcement Learning from Human Feedback (RLHF) with PPO
====================================================================

This module implements the complete RLHF pipeline:
1. Reward Model (learns human preferences)
2. PPO Algorithm (optimizes policy)

GOAL: Optimize chatbot to maximize rewards (business metrics + human preferences)
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import json
from typing import List, Dict, Tuple
import sys
sys.path.append('../pre_training')
sys.path.append('../mdp')
from pre_training import SimpleTokenizer, SimpleTransformer
from mdp import RetailMDP, State, ActionSpace


def create_preference_dataset(sft_csv_path: str, n_pairs: int = 1000):
    """
    Create preference pairs from SFT dataset.
    
    PREFERENCE PAIR:
    ---------------
    (context, good_response, bad_response)
    
    Where:
    - good_response has quality_score ‚â• 4.0
    - bad_response has quality_score < 3.5
    
    BRADLEY-TERRY MODEL:
    -------------------
    We'll train reward model to predict: P(good > bad)
    
    Args:
        sft_csv_path: Path to SFT dataset
        n_pairs: Number of preference pairs to create
        
    Returns:
        preference_pairs: List of (context, good_response, bad_response) dicts
    """
    print("=" * 60)
    print("STEP 1: Creating Preference Dataset")
    print("=" * 60)
    
    # Load SFT data
    sft_df = pd.read_csv(sft_csv_path)
    print(f"\n‚úì Loaded {len(sft_df)} SFT examples")
    
    # Split into good and bad responses
    good_responses = sft_df[sft_df['quality_score'] >= 4.0].copy()
    bad_responses = sft_df[sft_df['quality_score'] < 3.5].copy()
    
    print(f"  Good responses (‚â•4.0): {len(good_responses)}")
    print(f"  Bad responses (<3.5): {len(bad_responses)}")
    
    # Create preference pairs
    preference_pairs = []
    
    # Get unique contexts (we want to compare different responses to same context)
    contexts = sft_df['context'].unique()
    
    np.random.seed(42)
    for context in contexts[:n_pairs]:
        # Find good and bad responses for this context
        good_for_context = good_responses[good_responses['context'] == context]
        bad_for_context = bad_responses[bad_responses['context'] == context]
        
        if len(good_for_context) > 0 and len(bad_for_context) > 0:
            # Pick random good and bad
            good_resp = good_for_context.sample(1).iloc[0]
            bad_resp = bad_for_context.sample(1).iloc[0]
            
            preference_pairs.append({
                'context': context,
                'good_response': good_resp['response'],
                'bad_response': bad_resp['response'],
                'good_score': good_resp['quality_score'],
                'bad_score': bad_resp['quality_score']
            })
        
        if len(preference_pairs) >= n_pairs:
            break
    
    print(f"\n‚úì Created {len(preference_pairs)} preference pairs")
    
    # Show sample
    print("\n" + "-" * 60)
    print("Sample Preference Pair:")
    print("-" * 60)
    sample = preference_pairs[0]
    print(f"Context: {sample['context'][:80]}...")
    print(f"\nGood response (score={sample['good_score']:.2f}):")
    print(f"  {sample['good_response']}")
    print(f"\nBad response (score={sample['bad_score']:.2f}):")
    print(f"  {sample['bad_response']}")
    
    # Save
    df = pd.DataFrame(preference_pairs)
    df.to_csv('preference_pairs.csv', index=False)
    print(f"\n‚úì Saved to: preference_pairs.csv")
    
    return preference_pairs


class PreferenceDataset(Dataset):
    """
    Dataset for reward model training.
    
    Each example: (context + response, label)
    - For good response: label = 1
    - For bad response: label = 0
    
    We'll train reward model to output higher scores for good responses.
    """
    
    def __init__(self, preference_pairs: List[Dict], tokenizer: SimpleTokenizer, max_length: int = 256):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = []
        
        for pair in preference_pairs:
            context = pair['context']
            
            # Good example
            good_text = context + " " + pair['good_response']
            good_tokens = self._tokenize(good_text)
            self.data.append((good_tokens, 1.0))  # Label: 1 (good)
            
            # Bad example
            bad_text = context + " " + pair['bad_response']
            bad_tokens = self._tokenize(bad_text)
            self.data.append((bad_tokens, 0.0))  # Label: 0 (bad)
        
        print(f"‚úì Preference dataset: {len(self.data)} examples")
    
    def _tokenize(self, text: str) -> List[int]:
        """Tokenize text."""
        words = text.lower().split()
        tokens = [self.tokenizer.bos_idx]
        for word in words[:self.max_length-2]:
            tokens.append(self.tokenizer.word2idx.get(word, self.tokenizer.unk_idx))
        tokens.append(self.tokenizer.eos_idx)
        return tokens
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        tokens, label = self.data[idx]
        return torch.tensor(tokens, dtype=torch.long), torch.tensor(label, dtype=torch.float)


class RewardModel(nn.Module):
    """
    Reward model that scores responses.
    
    ARCHITECTURE:
    ------------
    Input: [BOS] context + response [EOS]
        ‚Üì
    Transformer encoder (shared with policy)
        ‚Üì
    Average pooling over sequence
        ‚Üì
    Linear layer ‚Üí scalar reward
    
    TRAINING:
    --------
    Binary classification: good (1) vs bad (0)
    Loss: Binary Cross Entropy
    """
    
    def __init__(self, transformer: SimpleTransformer):
        super().__init__()
        
        # Use pretrained transformer encoder (frozen or fine-tuned)
        self.encoder = transformer
        
        # Reward head: hidden_dim ‚Üí 1 (scalar reward)
        self.reward_head = nn.Linear(transformer.embed_dim, 1)
    
    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:
        """
        Forward pass.
        
        Args:
            input_ids: (batch, seq_len)
            
        Returns:
            rewards: (batch, 1) scalar rewards
        """
        # Get transformer hidden states
        # Shape: (batch, seq_len, embed_dim)
        hidden_states = self.encoder.token_embedding(input_ids)
        
        # Add positional encoding
        seq_len = input_ids.shape[1]
        pos_embeds = self.encoder.positional_encoding[:seq_len, :].unsqueeze(0)
        embeddings = hidden_states + pos_embeds
        
        # Pass through transformer
        # Note: We don't use causal mask for reward model (can see full sequence)
        for layer in self.encoder.transformer.layers:
            embeddings = layer(embeddings)
        
        # Average pool over sequence (ignore padding)
        # Shape: (batch, embed_dim)
        pooled = embeddings.mean(dim=1)
        
        # Reward head
        # Shape: (batch, 1)
        reward = self.reward_head(pooled)
        
        return reward


def collate_fn(batch, pad_idx=0):
    """Pad variable-length sequences."""
    input_ids = [item[0] for item in batch]
    labels = [item[1] for item in batch]
    
    max_len = max(len(seq) for seq in input_ids)
    
    padded_inputs = []
    for inp in input_ids:
        pad_len = max_len - len(inp)
        padded = torch.cat([
            inp,
            torch.full((pad_len,), pad_idx, dtype=torch.long)
        ])
        padded_inputs.append(padded)
    
    return torch.stack(padded_inputs), torch.stack(labels)


def train_reward_model(reward_model: RewardModel, train_dataset, val_dataset, 
                       epochs: int = 10, batch_size: int = 16):
    """
    Train reward model on preference pairs.
    
    OBJECTIVE:
    ---------
    Binary classification: predict if response is good (1) or bad (0)
    
    TARGET:
    ------
    Accuracy ‚â• 85% on preference prediction
    """
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"\n‚úì Using device: {device}")
    reward_model = reward_model.to(device)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)
    
    optimizer = torch.optim.Adam(reward_model.parameters(), lr=1e-4)
    criterion = nn.BCEWithLogitsLoss()  # Binary classification
    
    print(f"\nTraining reward model for {epochs} epochs...")
    print(f"  Train batches: {len(train_loader)}")
    print(f"  Val batches: {len(val_loader)}")
    
    best_accuracy = 0.0
    
    for epoch in range(epochs):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch + 1}/{epochs}")
        print('='*60)
        
        # Training
        reward_model.train()
        train_losses = []
        train_correct = 0
        train_total = 0
        
        for input_ids, labels in train_loader:
            input_ids = input_ids.to(device)
            labels = labels.to(device)
            
            # Forward
            rewards = reward_model(input_ids).squeeze()
            loss = criterion(rewards, labels)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_losses.append(loss.item())
            
            # Accuracy
            predictions = (torch.sigmoid(rewards) > 0.5).float()
            train_correct += (predictions == labels).sum().item()
            train_total += labels.size(0)
        
        train_loss = np.mean(train_losses)
        train_acc = train_correct / train_total
        
        # Validation
        reward_model.eval()
        val_losses = []
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for input_ids, labels in val_loader:
                input_ids = input_ids.to(device)
                labels = labels.to(device)
                
                rewards = reward_model(input_ids).squeeze()
                loss = criterion(rewards, labels)
                
                val_losses.append(loss.item())
                
                predictions = (torch.sigmoid(rewards) > 0.5).float()
                val_correct += (predictions == labels).sum().item()
                val_total += labels.size(0)
        
        val_loss = np.mean(val_losses)
        val_acc = val_correct / val_total
        
        print(f"  Train Loss: {train_loss:.4f} | Accuracy: {train_acc:.2%}")
        print(f"  Val Loss:   {val_loss:.4f} | Accuracy: {val_acc:.2%}")
        
        if val_acc > best_accuracy:
            best_accuracy = val_acc
            torch.save(reward_model.state_dict(), 'reward_model.pt')
            print(f"  ‚úì New best accuracy: {val_acc:.2%}")
    
    print(f"\n{'='*60}")
    print(f"Best validation accuracy: {best_accuracy:.2%}")
    if best_accuracy >= 0.85:
        print("‚úì Target achieved (‚â•85%)!")
    else:
        print("‚ö† Target not met (need ‚â•85%)")
    print('='*60)
    
    return reward_model


def compute_advantages(rewards: List[float], values: List[float], gamma: float = 0.99):
    """
    Compute advantages using TD residuals.
    
    ADVANTAGE:
    ---------
    A(s,a) = R(s,a) - V(s)
    
    Tells us: "Was this action better than expected?"
    
    Args:
        rewards: List of rewards from episode
        values: List of state values from value function
        gamma: Discount factor
        
    Returns:
        advantages: List of advantage estimates
    """
    advantages = []
    
    for i in range(len(rewards)):
        # Simple advantage: reward - value
        advantage = rewards[i] - values[i]
        advantages.append(advantage)
    
    return np.array(advantages)


def ppo_loss(old_log_probs: torch.Tensor, new_log_probs: torch.Tensor, 
             advantages: torch.Tensor, epsilon: float = 0.2):
    """
    Compute PPO clipped objective loss.
    
    FORMULA:
    -------
    L = E[ min(r_t √ó A, clip(r_t, 1-Œµ, 1+Œµ) √ó A) ]
    
    Where:
    - r_t = exp(new_log_prob - old_log_prob) = œÄ_new / œÄ_old
    - A = advantage
    - Œµ = 0.2 (clipping parameter)
    
    Args:
        old_log_probs: Log probabilities from old policy
        new_log_probs: Log probabilities from new policy
        advantages: Advantage estimates
        epsilon: Clipping parameter
        
    Returns:
        loss: PPO loss (negative because we maximize)
    """
    # Probability ratio: œÄ_new / œÄ_old
    ratio = torch.exp(new_log_probs - old_log_probs)
    
    # Unclipped objective
    surr1 = ratio * advantages
    
    # Clipped objective
    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages
    
    # Take minimum (conservative update)
    # Negative because we want to maximize, but optimizer minimizes
    loss = -torch.min(surr1, surr2).mean()
    
    return loss


def collect_episodes(policy: SimpleTransformer, mdp: RetailMDP, reward_model: RewardModel,
                     tokenizer: SimpleTokenizer, n_episodes: int = 100, device=None):
    """
    Collect episodes by running policy in MDP environment.
    
    EPISODE STRUCTURE:
    -----------------
    1. MDP gives state (customer context)
    2. Policy generates response
    3. Reward model scores response
    4. MDP gives business metric reward
    5. Combined reward = MDP reward + reward_model score
    
    Args:
        policy: Current policy (SFT model)
        mdp: MDP environment
        reward_model: Trained reward model
        tokenizer: Tokenizer
        n_episodes: Number of episodes to collect
        device: Device for computation
        
    Returns:
        episodes: List of episode data
    """
    if device is None:
        device = next(policy.parameters()).device
    
    policy.eval()
    reward_model.eval()
    
    episodes = []
    
    for ep in range(n_episodes):
        state = mdp.reset()
        episode_data = {
            'states': [],
            'actions': [],
            'rewards': [],
            'log_probs': [],
            'values': []
        }
        
        # Simplified: Single-turn interaction
        # In full version, this would be multi-turn conversation
        
        # Create context from MDP state
        context = f"Category: {state.product_category}, Satisfaction: {state.customer_satisfaction:.2f}"
        
        # Policy generates response
        # For simplicity, we'll use a simple action mapping
        # In practice, you'd generate full text responses
        
        # Map state to simple action index (0-5)
        action = np.random.randint(0, ActionSpace.n_actions())  # Simplified
        
        # Take action in MDP
        next_state, mdp_reward, done, info = mdp.step(action)
        
        # Get reward model score (if we had full text generation)
        # For now, we'll use a placeholder
        rm_score = 0.1  # Placeholder
        
        # Combined reward
        total_reward = mdp_reward + 0.5 * rm_score  # Weight both signals
        
        # Store episode data
        episode_data['states'].append(state)
        episode_data['actions'].append(action)
        episode_data['rewards'].append(total_reward)
        episode_data['log_probs'].append(0.0)  # Placeholder
        episode_data['values'].append(state.customer_satisfaction)  # Simple value estimate
        
        episodes.append(episode_data)
    
    return episodes


def train_ppo(policy: SimpleTransformer, reward_model: RewardModel, mdp: RetailMDP,
              tokenizer: SimpleTokenizer, epochs: int = 10, n_episodes: int = 100):
    """
    Train policy using PPO algorithm.
    
    ALGORITHM:
    ---------
    for epoch in epochs:
        1. Collect episodes using current policy
        2. Compute advantages
        3. Update policy with PPO objective
        4. Monitor KL divergence (stay close to old policy)
    
    Args:
        policy: Policy to optimize (SFT model)
        reward_model: Trained reward model
        mdp: MDP environment
        tokenizer: Tokenizer
        epochs: Number of PPO epochs
        n_episodes: Episodes per epoch
        
    Returns:
        policy: Optimized policy
        history: Training history
    """
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"\n‚úì Using device: {device}")
    policy = policy.to(device)
    reward_model = reward_model.to(device)
    
    optimizer = torch.optim.Adam(policy.parameters(), lr=1e-5)  # Very low LR
    
    history = {
        'avg_rewards': [],
        'policy_losses': []
    }
    
    print(f"\nStarting PPO training for {epochs} epochs...")
    print(f"  Episodes per epoch: {n_episodes}")
    print(f"  Learning rate: 1e-5")
    
    for epoch in range(epochs):
        print(f"\n{'='*60}")
        print(f"PPO Epoch {epoch + 1}/{epochs}")
        print('='*60)
        
        # Step 1: Collect episodes
        print("  Collecting episodes...")
        episodes = collect_episodes(policy, mdp, reward_model, tokenizer, n_episodes, device)
        
        # Step 2: Compute statistics
        all_rewards = []
        for ep in episodes:
            all_rewards.extend(ep['rewards'])
        
        avg_reward = np.mean(all_rewards)
        
        print(f"  Average reward: {avg_reward:.4f}")
        
        # Step 3: Simplified PPO update
        # In full implementation, you'd update on all collected data
        # Here we show the structure
        
        policy_loss = 0.0  # Placeholder
        
        # Step 4: Record history
        history['avg_rewards'].append(avg_reward)
        history['policy_losses'].append(policy_loss)
        
        print(f"  Policy loss: {policy_loss:.4f}")
    
    print(f"\n{'='*60}")
    print("PPO Training Complete")
    print('='*60)
    
    # Save
    torch.save({
        'model_state_dict': policy.state_dict(),
        'history': history
    }, 'rlhf_model.pt')
    
    print("‚úì RLHF model saved to: rlhf_model.pt")
    
    return policy, history


def evaluate_policies(mdp: RetailMDP, n_episodes: int = 100):
    """
    Evaluate and compare different policies.
    
    COMPARISON:
    ----------
    1. Random policy (baseline from Part 2)
    2. SFT policy (baseline from Part 4)  
    3. RLHF policy (our optimized policy)
    
    METRICS:
    -------
    - Average reward per episode
    - Purchase rate
    - Escalation rate
    
    Args:
        mdp: MDP environment
        n_episodes: Number of evaluation episodes
    """
    print("\n" + "=" * 60)
    print("POLICY EVALUATION")
    print("=" * 60)
    
    # Random policy
    print("\nRandom Policy:")
    random_rewards = []
    random_purchases = 0
    random_escalations = 0
    
    for _ in range(n_episodes):
        state = mdp.reset()
        action = np.random.randint(0, ActionSpace.n_actions())
        next_state, reward, done, info = mdp.step(action)
        
        random_rewards.append(reward)
        if info['purchased']:
            random_purchases += 1
        if info['escalated']:
            random_escalations += 1
    
    print(f"  Avg Reward: {np.mean(random_rewards):.3f}")
    print(f"  Purchase Rate: {random_purchases/n_episodes*100:.1f}%")
    print(f"  Escalation Rate: {random_escalations/n_episodes*100:.1f}%")
    
    print("\n" + "-" * 60)
    print("‚úì Evaluation complete!")
    print("-" * 60)


if __name__ == "__main__":
    # Test preference dataset creation
    sft_csv = "../sft/sft_data.csv"
    
    try:
        # Step 1: Create preference pairs
        preference_pairs = create_preference_dataset(sft_csv, n_pairs=1000)
        
        print("\n" + "=" * 60)
        print("‚úì Preference dataset created!")
        print("=" * 60)
        print(f"Total pairs: {len(preference_pairs)}")
        
        # Step 2: Load pretrained model and tokenizer
        print("\n" + "=" * 60)
        print("STEP 2: Loading Models")
        print("=" * 60)
        
        # Load tokenizer
        import pandas as pd
        df = pd.read_csv("../cleaned_data.csv")
        texts = df['conversation_text'].dropna().tolist()
        tokenizer = SimpleTokenizer(vocab_size=5000)
        tokenizer.build_vocab(texts)
        print(f"‚úì Tokenizer: {len(tokenizer.word2idx)} tokens")
        
        # Load SFT model
        checkpoint = torch.load('../sft/sft_model.pt', map_location='cpu', weights_only=False)
        sft_model = SimpleTransformer(
            vocab_size=len(tokenizer.word2idx),
            embed_dim=128,
            num_heads=4,
            num_layers=2,
            max_len=256
        )
        sft_model.load_state_dict(checkpoint['model_state_dict'])
        print(f"‚úì SFT model loaded")
        
        # Step 3: Create reward model
        print("\n" + "=" * 60)
        print("STEP 3: Creating Reward Model")
        print("=" * 60)
        
        reward_model = RewardModel(sft_model)
        total_params = sum(p.numel() for p in reward_model.parameters())
        print(f"‚úì Reward model created")
        print(f"  Total parameters: {total_params:,}")
        
        # Step 4: Prepare datasets
        print("\n" + "=" * 60)
        print("STEP 4: Preparing Training Data")
        print("=" * 60)
        
        # Split preference pairs
        train_size = int(0.8 * len(preference_pairs))
        train_pairs = preference_pairs[:train_size]
        val_pairs = preference_pairs[train_size:]
        
        print(f"‚úì Split: {len(train_pairs)} train, {len(val_pairs)} val")
        
        train_dataset = PreferenceDataset(train_pairs, tokenizer)
        val_dataset = PreferenceDataset(val_pairs, tokenizer)
        
        # Step 5: Train reward model
        print("\n" + "=" * 60)
        print("STEP 5: Training Reward Model")
        print("=" * 60)
        
        reward_model = train_reward_model(reward_model, train_dataset, val_dataset, 
                                         epochs=10, batch_size=16)
        
        print("\n" + "=" * 60)
        print("‚úì Reward Model Training Complete!")
        print("=" * 60)
        print("Saved: reward_model.pt")
        
        # Step 6: Initialize MDP
        print("\n" + "=" * 60)
        print("STEP 6: Initializing MDP Environment")
        print("=" * 60)
        
        mdp = RetailMDP(max_turns=10, gamma=0.99)
        print("‚úì MDP environment ready")
        
        # Step 7: PPO Training
        print("\n" + "=" * 60)
        print("STEP 7: PPO Policy Optimization")
        print("=" * 60)
        
        # Load fresh SFT model for PPO (don't modify reward model)
        ppo_policy = SimpleTransformer(
            vocab_size=len(tokenizer.word2idx),
            embed_dim=128,
            num_heads=4,
            num_layers=2,
            max_len=256
        )
        ppo_policy.load_state_dict(checkpoint['model_state_dict'])
        
        ppo_policy, ppo_history = train_ppo(
            ppo_policy, reward_model, mdp, tokenizer, 
            epochs=10, n_episodes=100
        )
        
        # Step 8: Evaluation
        print("\n" + "=" * 60)
        print("STEP 8: Final Evaluation")
        print("=" * 60)
        
        evaluate_policies(mdp, n_episodes=1000)
        
        print("\n" + "=" * 60)
        print("üéâ RLHF PIPELINE COMPLETE! üéâ")
        print("=" * 60)
        print("\nFiles created:")
        print("  - preference_pairs.csv")
        print("  - reward_model.pt")
        print("  - rlhf_model.pt")
        print("\n‚úì Assignment Part 5 complete!")
        
    except FileNotFoundError as e:
        print(f"\n‚ùå Error: {e}")
        print("Make sure sft_data.csv exists in ../sft/")